{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e1fb81ca-f3a5-41d2-be53-bb5dcceea7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import h5py\n",
    "import datetime\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "raw",
   "id": "40495ed3-f46b-456c-95af-3da085a8856c",
   "metadata": {},
   "source": [
    "La prossima funzione ci permette di generare i gruppi dei nostri file hdf5 leggendo la struttura dei json i quali saranno del tipo:\n",
    "{\n",
    "    \"a\": str,\n",
    "    \"b\": {\n",
    "        \"value\": number,\n",
    "        \"unit\": str\n",
    "    }\n",
    "    \"c\": {\n",
    "        \"m_def\": \"NX_class\",\n",
    "        \"name\":str,\n",
    "        ...\n",
    "    }\n",
    "}\n",
    "\n",
    "La funzione in pratica verrà usata quando raggiungendo il livello di \"c\" userà \"c\" per nominare il gruppo (infatti le chiavi dei json sono costruite in base a ciò che si aspetta il file nexus) e userà l'oggetto in \"m_def\" per prendere il TYPE. La posizione va assegnata ma c'è un trick utile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fd84de23-184b-4069-9a15-1550733d8ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_group_to_fill(TYPE, where, name):\n",
    "    group=where.create_group(name)\n",
    "    group.attrs[\"NX_class\"]=TYPE\n",
    "    return group"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1cfa82a1-de5d-4b6b-851d-de95748ed708",
   "metadata": {},
   "source": [
    "Adesso che sappiamo generare i gruppi possiamo in teoria preoccuparci di scrivere le quantità in questi gruppi. Potremmo avere quantità stringa (le più semplici) che vengono lette e scritte come dataset del gruppo. I valori numerici senza unità di misura possono essere trattati come le stringhe. Abbiamo poi la possibilità di avere grandezze scalari con unità di misura, oppure grandezze vettoriali (simili alle scalari ma con un versore direzione associato). Infine potremmo trovare altri gruppi come elementi di un altro gruppo per cui quando arriviamo a quel punto usiamo \"m_def\" come spiegato prima per generare un altro gruppo dentro il gruppo in cui stiamo lavorando (l'attuale where) e riprocediamo a scriverne le quantità modificando i dati che scrviamo (quelli del nuovo gruppo salvati da dati[row]) e scrivendoli nel gruppo restituito dalla funzione precedente. Iteriamo fino al completamento del json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3911ccbb-ff9a-47c0-91c8-f62bba088caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_data(dati, where):\n",
    "    for row in dati:\n",
    "        if (isinstance(dati[row], str) and row != \"m_def\") or isinstance(dati[row], (int, float, bool)):\n",
    "            where.create_dataset(row, data=dati[row])\n",
    "        elif isinstance(dati[row], dict):\n",
    "            if dati[row].keys() == {\"value\", \"unit\", \"direction\"}:\n",
    "                where.create_dataset(row, data=dati[row][\"value\"])\n",
    "                where[row].attrs[\"units\"]=dati[row][\"unit\"]\n",
    "                where[row].attrs[\"direction\"] = dati[row][\"direction\"]\n",
    "            elif dati[row].keys() <= {\"value\", \"unit\"}:\n",
    "                where.create_dataset(row, data=dati[row][\"value\"])\n",
    "                where[row].attrs[\"units\"]=dati[row][\"unit\"]\n",
    "            elif all(isinstance(x, str) for x in dati[row].values()) and \"m_def\" not in dati[row].keys():\n",
    "                values= list(dati[row].values())\n",
    "                keys=list(dati[row].keys())\n",
    "                print(values, keys)\n",
    "                nome_dataset = keys[0]\n",
    "                valore_dataset = values[0]\n",
    "                where.create_dataset(nome_dataset, data= valore_dataset)\n",
    "                for attr, inst in zip(keys[1:], values[1:]):\n",
    "                    where[nome_dataset].attrs[attr]=inst\n",
    "            elif \"m_def\" in dati[row].keys():\n",
    "                newwhere=create_group_to_fill(dati[row][\"m_def\"], where, row)\n",
    "                write_data(dati[row], newwhere)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "83a84d5b-40a2-45d0-94d4-a7286fe22f7c",
   "metadata": {},
   "source": [
    "Piccola funzione per leggere e caricare il contenuto di un json come dati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9d046928-b284-40a0-8dda-16c459abea7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aa', 'bb']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mio_dict={\"a\": \"aa\", \"b\": \"bb\"}\n",
    "\n",
    "list(mio_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9d788412-b242-4ff9-99ed-2985544c8bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_from_json(json_input, where):\n",
    "    with open(json_input, \"r\", encoding=\"utf-8\") as file:\n",
    "        dati = json.load(file)\n",
    "        write_data(dati, where)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "42663210-c2c9-4520-b6d8-a4ea8c11ce81",
   "metadata": {},
   "source": [
    "Funzione per aprire json multipli avendo comunque cura di aprire per primo il file contenente le informazioni dell'entry (il top level dei nostri nexus). E poi dovendo dare sempre un sample, uno strumento e uno user generiamo a monte i gruppi per quelle sottoentries così non serve m_def a inizio di ogni file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f11c47d9-dbe9-42eb-a856-75302b13e9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aprire_jsons(directory, entry):\n",
    "    for file in os.listdir(directory):\n",
    "        if Path(file).suffix == \".json\" and \"entry\" in file:\n",
    "            write_from_json(os.path.join(directory, file), entry)\n",
    "\n",
    "    for file in os.listdir(directory):\n",
    "        if Path(file).suffix == \".json\" and \"sample\" in file:\n",
    "            sample=create_group_to_fill(\"NXsample\", entry, \"sample\")\n",
    "            write_from_json(os.path.join(directory, file), sample)\n",
    "        if Path(file).suffix == \".json\" and \"user\" in file:\n",
    "            user=create_group_to_fill(\"NXuser\", entry, \"user\")\n",
    "            write_from_json(os.path.join(directory, file), user)\n",
    "        if Path(file).suffix == \".json\" and \"instrument\" in file:\n",
    "            instr=create_group_to_fill(\"NXinstrument\", entry, \"instrument\")\n",
    "            write_from_json(os.path.join(directory, file), instr)\n",
    "        if Path(file).suffix == \".json\" and \"measurement\" in file:\n",
    "            measure=create_group_to_fill( \"NXem_measurement\", entry, \"measurement\")\n",
    "            write_from_json(os.path.join(directory, file), measure)\n",
    "            "
   ]
  },
  {
   "cell_type": "raw",
   "id": "e8dba2fa-8c11-4d53-a0b4-476132efb6bd",
   "metadata": {},
   "source": [
    "Finalmente scriviamo il fil per l'afm leggendo i vari jsons nella directory attuale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "63c918b0-35b8-4c79-8114-366351e13332",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(\"afm_file_auto1.nxs\", \"w\") as f:\n",
    "    entry = f.create_group(\"entry\")\n",
    "    entry.attrs[\"NX_class\"] = \"NXentry\"\n",
    "    entry.create_dataset(\"definition\", data=\"NXafm\")\n",
    "    entry[\"definition\"].attrs[\"version\"] = \"v2024.02\"\n",
    "    entry[\"definition\"].attrs[\"URL\"] = \"https://github.com/FAIRmat-NFDI/nexus_definitions/blob/fairmat/contributed_definitions/NXafm.nxdl.xml\"\n",
    "    aprire_jsons(\"./jsons_for_afm/\" , entry)\n",
    "    #### Completiamo generando i link necessari\n",
    "    scan = entry[\"instrument\"].create_group (\"scan_environment\")\n",
    "    scan.attrs[\"NX_class\"] = \"NXenvironment\"\n",
    "    scan[\"height_piezo_sensor\"] = entry[\"instrument\"][\"height_piezo_sensor\"]\n",
    "    scan[\"XY_piezo_sensor\"] = entry[\"instrument\"][\"XY_piezo_sensor\"]\n",
    "    scan[\"tip_temp_sensor\"] = entry[\"instrument\"][\"tip_temp_sensor\"]\n",
    "    scan.create_dataset(\"tip_temp\", data=25)\n",
    "    scan[\"tip_temp\"].attrs[\"units\"] = \"celsius\"\n",
    "    #### Reproducibility indicators\n",
    "    reprind = entry.create_group(\"reproducibility_indicators\")\n",
    "    reprind.attrs[\"NX_class\"] = \"NXcollection\"\n",
    "    reprind[\"cantilever_oscillator\"] = entry[\"instrument\"][\"cantilever\"][\"cantilever_oscillator\"]\n",
    "    reprind[\"cantilever_tip_temp\"] = entry[\"instrument\"][\"scan_environment\"][\"tip_temp\"]\n",
    "    #### Resolution indicators\n",
    "    resind = entry.create_group(\"resolution_indicators\")\n",
    "    resind.attrs[\"NX_class\"] = \"NXcollection\"\n",
    "    resind[\"oscillator_excitation\"] = entry[\"instrument\"][\"cantilever\"][\"cantilever_oscillator\"][\"oscillator_excitation\"]\n",
    "    resind[\"cantilever_tip_temp\"] = entry[\"instrument\"][\"scan_environment\"][\"tip_temp\"]\n",
    "    resind[\"amplitude_excitation\"] = entry[\"instrument\"][\"cantilever\"][\"cantilever_oscillator\"][\"phase_lock_loop\"][\"amplitude_excitation\"]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aea53e17-85ff-4fb5-9e33-d20830626a24",
   "metadata": {},
   "source": [
    "Proviamo a fare una funzione anche per scrivere un file per il SEM"
   ]
  },
  {
   "cell_type": "raw",
   "id": "70f44e3c-2d71-491f-b0c9-32f503a435da",
   "metadata": {},
   "source": [
    "Nel SEM è importante caricare anche i dati provenienti dalle immagini per cui dobbiamo caricare i dati presenti nella cartella tiffs_for_sem. Creiamo la funzione per estrarre i metadati dall'immagine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "fa2f0708-6624-415f-8c3a-eac63c668ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data_from_tif(tif_image):\n",
    "    import tifffile as tf\n",
    "    import numpy as np\n",
    "    name = f\"{tif_image}\"\n",
    "    array = tf.imread(tif_image)\n",
    "    array = array.astype(np.float32)\n",
    "    array /= array.max()\n",
    "    tf.imwrite(\"copy.tif\", array)\n",
    "    return name, array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "41ed7b8a-6da9-4d62-b3fb-9a6d69ddfe11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['name of the program', 'version'] ['program', 'version']\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(\"sem_file_auto.nxs\", \"w\") as f:\n",
    "    entry = f.create_group(\"entry\")\n",
    "    entry.attrs[\"NX_class\"] = \"NXentry\"\n",
    "    entry.create_dataset(\"definition\", data=\"NXem\")\n",
    "    entry[\"definition\"].attrs[\"version\"] = \"v2024.02\"\n",
    "    entry[\"definition\"].attrs[\"URL\"] = \"https://github.com/FAIRmat-NFDI/nexus_definitions/blob/a85e10cd0289f4e44b0fec011ff54703e6705383/contributed_definitions/NXem.nxdl.xml\"\n",
    "    aprire_jsons(\"./jsons_for_sem/\", entry)\n",
    "    for idx, tif_file in enumerate(sorted(Path(\"./tiffs_for_sem\").glob(\"*.tif\"))):\n",
    "        img_name, img_data = extract_data_from_tif(tif_file)\n",
    "        image = entry[\"measurement\"][\"event\"].create_group(f\"image_{idx+1}\")\n",
    "        image.attrs[\"NX_class\"] = \"NXimage\"\n",
    "        image_2d = image.create_group(\"image_2d\")\n",
    "        image_2d.attrs[\"NX_class\"] = \"NXdata\"\n",
    "        image_2d.create_dataset(\"title\", data=img_name)\n",
    "        image_2d.create_dataset(\"real\", data=img_data)\n",
    "        image_2d[\"real\"].attrs[\"long_name\"] = \"SEM Image Intensity\"\n",
    "        image_2d.create_dataset(\"axis_i\", data=np.arange(img_data.shape[0]))\n",
    "        image_2d.create_dataset(\"axis_j\", data=np.arange(img_data.shape[1]))\n",
    "        image_2d.attrs[\"signal\"] = \"real\"\n",
    "        image_2d.attrs[\"axes\"] = [\"axis_i\", \"axis_j\"]\n",
    "        image_2d.attrs[\"axis_i_indices\"] = 0\n",
    "        image_2d.attrs[\"axis_j_indices\"] = 1\n",
    "\n",
    "############ Comunque grandi passi avanti mi manca di potere visualizzare l'immagine su nomad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "350daa26-b27a-4570-b2d4-4ac19cf26efb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root:NXroot\n",
      "  entry:NXentry\n",
      "    coordinate_system:NXcoordinate_system\n",
      "      handedness = 'right_handed'\n",
      "      origin = 'sample'\n",
      "      type = 'cartesian'\n",
      "    definition = 'NXem'\n",
      "      @URL = 'https://github.com/FAIRmat-NFDI/nexus_definiti...'\n",
      "      @version = 'v2024.02'\n",
      "    end_time = ''\n",
      "    experiment_alias = 'alias'\n",
      "    experiment_description = 'short description'\n",
      "    experiment_identifier = 'experiment_identifier'\n",
      "    measurement:NXem_measurement\n",
      "      event:NXem_event_data\n",
      "        image:NX_image\n",
      "          image_2d:NXdata\n",
      "            title = ''\n",
      "          instrument:NXem_instrument\n",
      "          optics:NXem_optical_system\n",
      "            dynamic_focus_correction = True\n",
      "            field_of_view = 4\n",
      "              @units = 'm'\n",
      "            magnification = 0\n",
      "            probe_current = 4\n",
      "              @units = 'A'\n",
      "            tilt_correction = True\n",
      "            working_distance = 4\n",
      "              @units = 'm'\n",
      "        image_1:NXimage\n",
      "          image_2d:NXdata\n",
      "            @axes = ['axis_i', 'axis_j']\n",
      "            @axis_i_indices = 0\n",
      "            @axis_j_indices = 1\n",
      "            @signal = 'real'\n",
      "            axis_i = int64(1094)\n",
      "            axis_j = int64(1536)\n",
      "            real = float32(1094x1536)\n",
      "              @long_name = 'SEM Image Intensity'\n",
      "            title = 'tiffs_for_sem/U1502_metal_cross_5um_015.tif'\n",
      "        image_2:NXimage\n",
      "          image_2d:NXdata\n",
      "            @axes = ['axis_i', 'axis_j']\n",
      "            @axis_i_indices = 0\n",
      "            @axis_j_indices = 1\n",
      "            @signal = 'real'\n",
      "            axis_i = int64(1331)\n",
      "            axis_j = int64(1536)\n",
      "            real = float32(1331x1536)\n",
      "              @long_name = 'SEM Image Intensity'\n",
      "            title = 'tiffs_for_sem/run2_area_01.tif'\n",
      "      instrument:NXem_instrument\n",
      "        detector:NXdetector\n",
      "          type = 'type of the detector'\n",
      "        ebeam_column:NXebeam_column\n",
      "          electron_source:NXsource\n",
      "            emitter_material = 'emitter_material'\n",
      "            emitter_type = 'emitter_type'\n",
      "            probe = 'electrons'\n",
      "          fabrication:NXfabrication\n",
      "            model = 'model ebeam'\n",
      "            serial_number = ''\n",
      "            vendor = ''\n",
      "        fabrication:NXfabrication\n",
      "          manufacturer = 'manufacturer'\n",
      "          model = 'sem_model'\n",
      "          vendor = 'vendor'\n",
      "        location = 'Trento'\n",
      "        name = 'nome sem'\n",
      "        program:NXprogram\n",
      "          program = 'name of the program'\n",
      "            @version = 'version'\n",
      "        type = 'pfib'\n",
      "    sample:NXsample\n",
      "      atom_types = 'Si, O'\n",
      "      identifier_sample = 'sample_ID'\n",
      "      is_simulation = True\n",
      "      name = 'Sample SEM'\n",
      "      physical_form = 'bulk'\n",
      "      preparation_date = ''\n",
      "    start_time = ''\n",
      "    user:NXuser\n",
      "      ORCID = '0009-0004-3810-1808'\n",
      "      address = 'Povo(TN)'\n",
      "      affiliation = 'FBK'\n",
      "      email = 'mbontorno@fbk.eu'\n",
      "      facility_user_id = 'MB'\n",
      "      name = 'Matteo Bontorno'\n",
      "      role = 'Collaborator'\n"
     ]
    }
   ],
   "source": [
    "from nexusformat.nexus import *\n",
    "\n",
    "test = nxload(\"sem_file_auto.nxs\")\n",
    "print(test.tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a87407-075b-4f25-95aa-26d0d8d9bd7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec5af5d-61c5-4669-8307-e28da59d453d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857d9958-345e-4700-bfbc-f090251e8360",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a2612c82-7635-4b1e-927a-284d8a398ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1094, 1536) uint16\n"
     ]
    }
   ],
   "source": [
    "import tifffile as tf\n",
    "import numpy as np\n",
    "image = tf.imread('./tiffs_for_sem/U1502_metal_cross_5um_015.tif')\n",
    "image = image.astype(np.float32)\n",
    "image /= image.max()\n",
    "print(image.shape, image.dtype)\n",
    "\n",
    "tf.imwrite(\"copy.tif\", image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9416b41-6133-4e6e-bbe5-97b5d0b403ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1fb74970-294d-4586-b612-77a3efbf3838",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tifffile\n",
    "import json\n",
    "\n",
    "def leggi_metadati_tif(percorso):\n",
    "    metadati_pagine = []\n",
    "    with tifffile.TiffFile(percorso) as tif:\n",
    "        for i, page in enumerate(tif.pages):\n",
    "            # Convertiamo i tag in dict leggibile\n",
    "            tags = {}\n",
    "            for tag in page.tags.values():\n",
    "                try:\n",
    "                    tags[tag.name] = str(tag.value)\n",
    "                except Exception:\n",
    "                    tags[tag.name] = \"Non leggibile\"\n",
    "            \n",
    "            metadati_pagine.append({\n",
    "                \"Pagina\": i + 1,\n",
    "                \"Dimensioni immagine\": page.shape,\n",
    "                \"Tipo dati\": page.dtype.name,\n",
    "                \"Descrizione\": page.description,\n",
    "                \"Tag\": tags\n",
    "            })\n",
    "    \n",
    "    return {\n",
    "        \"Nome file\": percorso.split(\"/\")[-1],\n",
    "        \"Pagine\": metadati_pagine\n",
    "    }\n",
    "\n",
    "# Lista dei file TIFF\n",
    "file_paths = [\n",
    "    \"./tiffs_for_sem/run2_area_01.tif\",\n",
    "    \"./tiffs_for_sem/U1502_metal_cross_5um_015.tif\"\n",
    "]\n",
    "\n",
    "# Creiamo il JSON usando il nome del file come chiave\n",
    "metadati = {fp.split(\"/\")[-1]: leggi_metadati_tif(fp) for fp in file_paths}\n",
    "\n",
    "# Salvataggio in JSON\n",
    "with open(\"metadati_tif.json\", \"w\") as f:\n",
    "    json.dump(metadati, f, indent=4, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f2640fc-9452-4879-a29b-c79da5bd6d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tifffile\n",
    "import json\n",
    "\n",
    "def leggi_metadati_tif(percorso):\n",
    "    with tifffile.TiffFile(percorso) as tif:\n",
    "        page = tif.pages[0]\n",
    "        return {\n",
    "            \"Nome file\": percorso.split(\"/\")[-1],\n",
    "            \"Dimensioni immagine\": page.shape,\n",
    "            \"Tipo dati\": page.dtype.name,\n",
    "            \"Descrizione\": page.description,\n",
    "            \"Tag\": {tag.name: tag.value for tag in page.tags.values()}\n",
    "        }\n",
    "\n",
    "# Uso con più file\n",
    "file_paths = [\n",
    "    \"./tiffs_for_sem/run2_area_01.tif\",\n",
    "    \"./tiffs_for_sem/U1502_metal_cross_5um_015.tif\"\n",
    "]\n",
    "\n",
    "metadati = {f\"file{i+1}\": leggi_metadati_tif(fp) for i, fp in enumerate(file_paths)}\n",
    "\n",
    "# Salvataggio in JSON\n",
    "with open(\"metadati_tif.json\", \"w\") as f:\n",
    "    json.dump(metadati, f, indent=4, default=str)\n",
    "\n",
    "### Da sistemare e rendere più generale possibile per leggere ogni forma di tiff\n",
    "## Penso che comunque l'approccio migliore sia fare una funzione per ogni macchina\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb111400-1953-4a68-9007-6262bcf55ce9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
